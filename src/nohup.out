INFO:root:Log file is ../log/mnist_test/log.txt.
INFO:root:Data path is ../data.
INFO:root:Export path is ../log/mnist_test.
INFO:root:Dataset: mnist
INFO:root:Normal class: 3
INFO:root:Network: mnist_LeNet
INFO:root:Deep SVDD objective: one-class
INFO:root:Nu-paramerter: 0.10
INFO:root:Computation device: cuda
INFO:root:Number of dataloader workers: 0
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
INFO:root:Pretraining: True
INFO:root:Pretraining optimizer: adam
INFO:root:Pretraining learning rate: 0.0001
INFO:root:Pretraining epochs: 150
INFO:root:Pretraining learning rate scheduler milestones: (50,)
INFO:root:Pretraining batch size: 200
INFO:root:Pretraining weight decay: 0.0005
INFO:root:Starting pretraining...
/home/liujh/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data
  warnings.warn("train_data has been renamed data")
INFO:root:  Epoch 1/150	 Time: 1.846	 Loss: 171.45385742
INFO:root:  Epoch 2/150	 Time: 1.868	 Loss: 130.46922794
INFO:root:  Epoch 3/150	 Time: 2.050	 Loss: 94.72893057
INFO:root:  Epoch 4/150	 Time: 1.977	 Loss: 71.61579784
INFO:root:  Epoch 5/150	 Time: 1.866	 Loss: 55.07808439
INFO:root:  Epoch 6/150	 Time: 1.936	 Loss: 42.46604649
INFO:root:  Epoch 7/150	 Time: 4.753	 Loss: 34.46776815
INFO:root:  Epoch 8/150	 Time: 2.226	 Loss: 29.00169163
INFO:root:  Epoch 9/150	 Time: 1.974	 Loss: 24.96931704
INFO:root:  Epoch 10/150	 Time: 1.935	 Loss: 21.94582859
INFO:root:  Epoch 11/150	 Time: 1.820	 Loss: 19.69717801
INFO:root:  Epoch 12/150	 Time: 2.014	 Loss: 17.93485463
INFO:root:  Epoch 13/150	 Time: 2.059	 Loss: 16.49294090
INFO:root:  Epoch 14/150	 Time: 4.548	 Loss: 15.29109752
INFO:root:  Epoch 15/150	 Time: 1.914	 Loss: 14.28105308
INFO:root:  Epoch 16/150	 Time: 1.928	 Loss: 13.41027401
INFO:root:  Epoch 17/150	 Time: 1.943	 Loss: 12.63934286
INFO:root:  Epoch 18/150	 Time: 1.785	 Loss: 11.95904412
INFO:root:  Epoch 19/150	 Time: 1.768	 Loss: 11.35306389
INFO:root:  Epoch 20/150	 Time: 1.780	 Loss: 10.80184903
INFO:root:  Epoch 21/150	 Time: 4.890	 Loss: 10.29772872
INFO:root:  Epoch 22/150	 Time: 1.824	 Loss: 9.83744424
INFO:root:  Epoch 23/150	 Time: 1.845	 Loss: 9.40908146
INFO:root:  Epoch 24/150	 Time: 1.799	 Loss: 9.00975649
INFO:root:  Epoch 25/150	 Time: 1.790	 Loss: 8.63548931
INFO:root:  Epoch 26/150	 Time: 1.859	 Loss: 8.28169254
INFO:root:  Epoch 27/150	 Time: 1.816	 Loss: 7.96591174
INFO:root:  Epoch 28/150	 Time: 4.633	 Loss: 7.67370261
INFO:root:  Epoch 29/150	 Time: 2.006	 Loss: 7.41794909
INFO:root:  Epoch 30/150	 Time: 2.059	 Loss: 7.18890436
INFO:root:  Epoch 31/150	 Time: 1.826	 Loss: 6.97939453
INFO:root:  Epoch 32/150	 Time: 2.096	 Loss: 6.79191854
INFO:root:  Epoch 33/150	 Time: 1.921	 Loss: 6.62176277
INFO:root:  Epoch 34/150	 Time: 1.870	 Loss: 6.47056575
INFO:root:  Epoch 35/150	 Time: 4.123	 Loss: 6.32820420
INFO:root:  Epoch 36/150	 Time: 2.098	 Loss: 6.19661422
INFO:root:  Epoch 37/150	 Time: 1.912	 Loss: 6.07661824
INFO:root:  Epoch 38/150	 Time: 1.844	 Loss: 5.96289978
INFO:root:  Epoch 39/150	 Time: 1.786	 Loss: 5.85446338
INFO:root:  Epoch 40/150	 Time: 1.767	 Loss: 5.75008368
INFO:root:  Epoch 41/150	 Time: 1.723	 Loss: 5.64335560
INFO:root:  Epoch 42/150	 Time: 2.721	 Loss: 5.54556831
INFO:root:  Epoch 43/150	 Time: 3.859	 Loss: 5.44672048
INFO:root:  Epoch 44/150	 Time: 1.926	 Loss: 5.36282258
INFO:root:  Epoch 45/150	 Time: 2.043	 Loss: 5.28018041
INFO:root:  Epoch 46/150	 Time: 1.942	 Loss: 5.20474408
INFO:root:  Epoch 47/150	 Time: 1.733	 Loss: 5.13727534
INFO:root:  Epoch 48/150	 Time: 1.770	 Loss: 5.07152528
INFO:root:  Epoch 49/150	 Time: 3.007	 Loss: 5.01035078
INFO:root:  Epoch 50/150	 Time: 3.634	 Loss: 4.97649473
INFO:root:  LR scheduler: new learning rate is 1e-05
INFO:root:  Epoch 51/150	 Time: 2.014	 Loss: 4.97162724
INFO:root:  Epoch 52/150	 Time: 1.903	 Loss: 4.96430763
INFO:root:  Epoch 53/150	 Time: 2.013	 Loss: 4.96225303
INFO:root:  Epoch 54/150	 Time: 1.837	 Loss: 4.95304140
INFO:root:  Epoch 55/150	 Time: 1.779	 Loss: 4.94917060
INFO:root:  Epoch 56/150	 Time: 3.920	 Loss: 4.93927016
INFO:root:  Epoch 57/150	 Time: 2.570	 Loss: 4.93760704
INFO:root:  Epoch 58/150	 Time: 1.783	 Loss: 4.92957258
INFO:root:  Epoch 59/150	 Time: 1.798	 Loss: 4.91866813
INFO:root:  Epoch 60/150	 Time: 1.838	 Loss: 4.91429164
INFO:root:  Epoch 61/150	 Time: 1.884	 Loss: 4.90920448
INFO:root:  Epoch 62/150	 Time: 1.799	 Loss: 4.90366360
INFO:root:  Epoch 63/150	 Time: 1.816	 Loss: 4.89588265
INFO:root:  Epoch 64/150	 Time: 4.628	 Loss: 4.89103334
INFO:root:  Epoch 65/150	 Time: 2.162	 Loss: 4.88444305
INFO:root:  Epoch 66/150	 Time: 1.757	 Loss: 4.87987438
INFO:root:  Epoch 67/150	 Time: 1.787	 Loss: 4.87060095
INFO:root:  Epoch 68/150	 Time: 1.804	 Loss: 4.86258708
INFO:root:  Epoch 69/150	 Time: 1.746	 Loss: 4.85776355
INFO:root:  Epoch 70/150	 Time: 1.737	 Loss: 4.85235325
INFO:root:  Epoch 71/150	 Time: 4.487	 Loss: 4.84566213
INFO:root:  Epoch 72/150	 Time: 1.793	 Loss: 4.83666069
INFO:root:  Epoch 73/150	 Time: 1.776	 Loss: 4.83129215
INFO:root:  Epoch 74/150	 Time: 1.879	 Loss: 4.82419846
INFO:root:  Epoch 75/150	 Time: 1.785	 Loss: 4.81478039
INFO:root:  Epoch 76/150	 Time: 1.729	 Loss: 4.81025822
INFO:root:  Epoch 77/150	 Time: 1.817	 Loss: 4.80204804
INFO:root:  Epoch 78/150	 Time: 4.642	 Loss: 4.79564295
INFO:root:  Epoch 79/150	 Time: 2.135	 Loss: 4.78893955
INFO:root:  Epoch 80/150	 Time: 1.899	 Loss: 4.78075795
INFO:root:  Epoch 81/150	 Time: 1.827	 Loss: 4.77611969
INFO:root:  Epoch 82/150	 Time: 1.796	 Loss: 4.76741795
INFO:root:  Epoch 83/150	 Time: 1.852	 Loss: 4.76128601
INFO:root:  Epoch 84/150	 Time: 1.738	 Loss: 4.75316309
INFO:root:  Epoch 85/150	 Time: 3.539	 Loss: 4.74692185
INFO:root:  Epoch 86/150	 Time: 2.818	 Loss: 4.73739352
INFO:root:  Epoch 87/150	 Time: 1.865	 Loss: 4.73170014
INFO:root:  Epoch 88/150	 Time: 1.861	 Loss: 4.72489805
INFO:root:  Epoch 89/150	 Time: 1.922	 Loss: 4.71626645
INFO:root:  Epoch 90/150	 Time: 1.812	 Loss: 4.70775129
INFO:root:  Epoch 91/150	 Time: 1.744	 Loss: 4.70344928
INFO:root:  Epoch 92/150	 Time: 1.767	 Loss: 4.69412145
INFO:root:  Epoch 93/150	 Time: 4.783	 Loss: 4.68703834
INFO:root:  Epoch 94/150	 Time: 2.055	 Loss: 4.68320522
INFO:root:  Epoch 95/150	 Time: 1.865	 Loss: 4.67175104
INFO:root:  Epoch 96/150	 Time: 1.790	 Loss: 4.66526816
INFO:root:  Epoch 97/150	 Time: 1.775	 Loss: 4.65391190
INFO:root:  Epoch 98/150	 Time: 1.890	 Loss: 4.65116593
INFO:root:  Epoch 99/150	 Time: 1.876	 Loss: 4.64425325
INFO:root:  Epoch 100/150	 Time: 4.672	 Loss: 4.63206733
INFO:root:  Epoch 101/150	 Time: 1.982	 Loss: 4.62730542
INFO:root:  Epoch 102/150	 Time: 1.895	 Loss: 4.62039092
INFO:root:  Epoch 103/150	 Time: 1.815	 Loss: 4.61086947
INFO:root:  Epoch 104/150	 Time: 1.930	 Loss: 4.60515790
INFO:root:  Epoch 105/150	 Time: 1.733	 Loss: 4.59688762
INFO:root:  Epoch 106/150	 Time: 1.900	 Loss: 4.59064914
INFO:root:  Epoch 107/150	 Time: 4.563	 Loss: 4.58292543
INFO:root:  Epoch 108/150	 Time: 2.190	 Loss: 4.57563248
INFO:root:  Epoch 109/150	 Time: 1.845	 Loss: 4.56515198
INFO:root:  Epoch 110/150	 Time: 1.885	 Loss: 4.55750718
INFO:root:  Epoch 111/150	 Time: 1.830	 Loss: 4.55054339
INFO:root:  Epoch 112/150	 Time: 1.821	 Loss: 4.54201005
INFO:root:  Epoch 113/150	 Time: 1.888	 Loss: 4.53536987
INFO:root:  Epoch 114/150	 Time: 5.178	 Loss: 4.52600118
INFO:root:  Epoch 115/150	 Time: 1.940	 Loss: 4.52047373
INFO:root:  Epoch 116/150	 Time: 1.938	 Loss: 4.50984978
INFO:root:  Epoch 117/150	 Time: 1.870	 Loss: 4.50294976
INFO:root:  Epoch 118/150	 Time: 1.841	 Loss: 4.49255603
INFO:root:  Epoch 119/150	 Time: 1.752	 Loss: 4.48463048
INFO:root:  Epoch 120/150	 Time: 1.821	 Loss: 4.47530905
INFO:root:  Epoch 121/150	 Time: 4.825	 Loss: 4.46852149
INFO:root:  Epoch 122/150	 Time: 2.051	 Loss: 4.46187258
INFO:root:  Epoch 123/150	 Time: 1.888	 Loss: 4.45377801
INFO:root:  Epoch 124/150	 Time: 1.899	 Loss: 4.44572759
INFO:root:  Epoch 125/150	 Time: 1.891	 Loss: 4.43795441
INFO:root:  Epoch 126/150	 Time: 2.007	 Loss: 4.43333557
INFO:root:  Epoch 127/150	 Time: 4.702	 Loss: 4.41873143
INFO:root:  Epoch 128/150	 Time: 2.373	 Loss: 4.41137485
INFO:root:  Epoch 129/150	 Time: 1.809	 Loss: 4.40520643
INFO:root:  Epoch 130/150	 Time: 1.927	 Loss: 4.39711786
INFO:root:  Epoch 131/150	 Time: 1.792	 Loss: 4.39063011
INFO:root:  Epoch 132/150	 Time: 1.744	 Loss: 4.38027197
INFO:root:  Epoch 133/150	 Time: 1.714	 Loss: 4.37316442
INFO:root:  Epoch 134/150	 Time: 4.548	 Loss: 4.36425640
INFO:root:  Epoch 135/150	 Time: 1.890	 Loss: 4.35656320
INFO:root:  Epoch 136/150	 Time: 1.955	 Loss: 4.34848365
INFO:root:  Epoch 137/150	 Time: 1.848	 Loss: 4.34133244
INFO:root:  Epoch 138/150	 Time: 1.861	 Loss: 4.33344735
INFO:root:  Epoch 139/150	 Time: 1.774	 Loss: 4.32275475
INFO:root:  Epoch 140/150	 Time: 1.753	 Loss: 4.31501935
INFO:root:  Epoch 141/150	 Time: 1.818	 Loss: 4.30854830
INFO:root:  Epoch 142/150	 Time: 4.819	 Loss: 4.29778596
INFO:root:  Epoch 143/150	 Time: 1.837	 Loss: 4.29077659
INFO:root:  Epoch 144/150	 Time: 1.862	 Loss: 4.28209528
INFO:root:  Epoch 145/150	 Time: 1.793	 Loss: 4.27521919
INFO:root:  Epoch 146/150	 Time: 1.782	 Loss: 4.26888912
INFO:root:  Epoch 147/150	 Time: 1.770	 Loss: 4.25676896
INFO:root:  Epoch 148/150	 Time: 1.723	 Loss: 4.24804960
INFO:root:  Epoch 149/150	 Time: 4.556	 Loss: 4.23961619
INFO:root:  Epoch 150/150	 Time: 2.273	 Loss: 4.23558686
INFO:root:Pretraining time: 340.016
INFO:root:Finished pretraining.
INFO:root:Testing autoencoder...
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data
  warnings.warn("test_data has been renamed data")
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets
  warnings.warn("test_labels has been renamed targets")
INFO:root:Test set Loss: 6.53357277
INFO:root:Test set AUC: 85.69%
INFO:root:Autoencoder testing time: 2.775
INFO:root:Finished testing autoencoder.
INFO:root:Training optimizer: adam
INFO:root:Training learning rate: 0.0001
INFO:root:Training epochs: 150
INFO:root:Training learning rate scheduler milestones: (50,)
INFO:root:Training batch size: 200
INFO:root:Training weight decay: 5e-07
INFO:root:Initializing center c...
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:53: UserWarning: train_data has been renamed data
  warnings.warn("train_data has been renamed data")
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:43: UserWarning: train_labels has been renamed targets
  warnings.warn("train_labels has been renamed targets")
INFO:root:Center c initialized.
INFO:root:Starting training...
/home/liujh/.local/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:82: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
INFO:root:  Epoch 1/150	 Time: 1.783	 Loss: 1.36185184
INFO:root:  Epoch 2/150	 Time: 1.762	 Loss: 0.80506066
INFO:root:  Epoch 3/150	 Time: 2.545	 Loss: 0.52068036
INFO:root:  Epoch 4/150	 Time: 4.192	 Loss: 0.36537753
INFO:root:  Epoch 5/150	 Time: 1.823	 Loss: 0.27291220
INFO:root:  Epoch 6/150	 Time: 1.750	 Loss: 0.21568397
INFO:root:  Epoch 7/150	 Time: 1.791	 Loss: 0.17697627
INFO:root:  Epoch 8/150	 Time: 1.804	 Loss: 0.14918413
INFO:root:  Epoch 9/150	 Time: 1.746	 Loss: 0.13019758
INFO:root:  Epoch 10/150	 Time: 1.713	 Loss: 0.11525720
INFO:root:  Epoch 11/150	 Time: 4.859	 Loss: 0.10448485
INFO:root:  Epoch 12/150	 Time: 1.964	 Loss: 0.09717744
INFO:root:  Epoch 13/150	 Time: 1.989	 Loss: 0.08756142
INFO:root:  Epoch 14/150	 Time: 1.728	 Loss: 0.07958023
INFO:root:  Epoch 15/150	 Time: 1.689	 Loss: 0.07423669
INFO:root:  Epoch 16/150	 Time: 1.730	 Loss: 0.07049484
INFO:root:  Epoch 17/150	 Time: 1.794	 Loss: 0.06585386
INFO:root:  Epoch 18/150	 Time: 4.568	 Loss: 0.06068716
INFO:root:  Epoch 19/150	 Time: 1.985	 Loss: 0.05816147
INFO:root:  Epoch 20/150	 Time: 1.771	 Loss: 0.05565598
INFO:root:  Epoch 21/150	 Time: 1.792	 Loss: 0.05212510
INFO:root:  Epoch 22/150	 Time: 1.744	 Loss: 0.04921182
INFO:root:  Epoch 23/150	 Time: 1.722	 Loss: 0.04796688
INFO:root:  Epoch 24/150	 Time: 1.703	 Loss: 0.04485248
INFO:root:  Epoch 25/150	 Time: 3.219	 Loss: 0.04327352
INFO:root:  Epoch 26/150	 Time: 2.751	 Loss: 0.03981091
INFO:root:  Epoch 27/150	 Time: 1.877	 Loss: 0.04101890
INFO:root:  Epoch 28/150	 Time: 1.820	 Loss: 0.03937350
INFO:root:  Epoch 29/150	 Time: 1.765	 Loss: 0.03947296
INFO:root:  Epoch 30/150	 Time: 1.716	 Loss: 0.03690043
INFO:root:  Epoch 31/150	 Time: 1.743	 Loss: 0.03549120
INFO:root:  Epoch 32/150	 Time: 1.693	 Loss: 0.03419450
INFO:root:  Epoch 33/150	 Time: 4.830	 Loss: 0.03228799
INFO:root:  Epoch 34/150	 Time: 1.753	 Loss: 0.03276381
INFO:root:  Epoch 35/150	 Time: 1.697	 Loss: 0.03156315
INFO:root:  Epoch 36/150	 Time: 1.730	 Loss: 0.03016131
INFO:root:  Epoch 37/150	 Time: 1.896	 Loss: 0.02801094
INFO:root:  Epoch 38/150	 Time: 1.955	 Loss: 0.02730713
INFO:root:  Epoch 39/150	 Time: 1.901	 Loss: 0.02862177
INFO:root:  Epoch 40/150	 Time: 4.495	 Loss: 0.02641646
INFO:root:  Epoch 41/150	 Time: 1.759	 Loss: 0.02535571
INFO:root:  Epoch 42/150	 Time: 1.869	 Loss: 0.02649610
INFO:root:  Epoch 43/150	 Time: 1.742	 Loss: 0.02451380
INFO:root:  Epoch 44/150	 Time: 1.734	 Loss: 0.02397033
INFO:root:  Epoch 45/150	 Time: 1.724	 Loss: 0.02380169
INFO:root:  Epoch 46/150	 Time: 1.893	 Loss: 0.02372668
INFO:root:  Epoch 47/150	 Time: 1.802	 Loss: 0.02329866
INFO:root:  Epoch 48/150	 Time: 4.913	 Loss: 0.02345480
INFO:root:  Epoch 49/150	 Time: 2.066	 Loss: 0.02101218
INFO:root:  Epoch 50/150	 Time: 1.811	 Loss: 0.02007822
INFO:root:  LR scheduler: new learning rate is 1e-05
INFO:root:  Epoch 51/150	 Time: 1.763	 Loss: 0.02102636
INFO:root:  Epoch 52/150	 Time: 1.773	 Loss: 0.02067179
INFO:root:  Epoch 53/150	 Time: 1.840	 Loss: 0.02039014
INFO:root:  Epoch 54/150	 Time: 1.810	 Loss: 0.01983968
INFO:root:  Epoch 55/150	 Time: 4.173	 Loss: 0.02122985
INFO:root:  Epoch 56/150	 Time: 1.879	 Loss: 0.02121788
INFO:root:  Epoch 57/150	 Time: 1.811	 Loss: 0.02057711
INFO:root:  Epoch 58/150	 Time: 1.852	 Loss: 0.02036477
INFO:root:  Epoch 59/150	 Time: 1.892	 Loss: 0.02139602
INFO:root:  Epoch 60/150	 Time: 1.880	 Loss: 0.02086978
INFO:root:  Epoch 61/150	 Time: 1.861	 Loss: 0.02074634
INFO:root:  Epoch 62/150	 Time: 4.533	 Loss: 0.02028877
INFO:root:  Epoch 63/150	 Time: 2.134	 Loss: 0.01939810
INFO:root:  Epoch 64/150	 Time: 1.741	 Loss: 0.02095770
INFO:root:  Epoch 65/150	 Time: 1.779	 Loss: 0.01978500
INFO:root:  Epoch 66/150	 Time: 1.889	 Loss: 0.02050736
INFO:root:  Epoch 67/150	 Time: 1.926	 Loss: 0.02028563
INFO:root:  Epoch 68/150	 Time: 1.952	 Loss: 0.02033080
INFO:root:  Epoch 69/150	 Time: 4.242	 Loss: 0.01971955
INFO:root:  Epoch 70/150	 Time: 1.874	 Loss: 0.01957638
INFO:root:  Epoch 71/150	 Time: 1.883	 Loss: 0.02045675
INFO:root:  Epoch 72/150	 Time: 1.717	 Loss: 0.01950574
INFO:root:  Epoch 73/150	 Time: 1.809	 Loss: 0.01947513
INFO:root:  Epoch 74/150	 Time: 1.770	 Loss: 0.01929700
INFO:root:  Epoch 75/150	 Time: 1.710	 Loss: 0.02001633
INFO:root:  Epoch 76/150	 Time: 1.705	 Loss: 0.02061197
INFO:root:  Epoch 77/150	 Time: 4.661	 Loss: 0.01989819
INFO:root:  Epoch 78/150	 Time: 1.955	 Loss: 0.01933205
INFO:root:  Epoch 79/150	 Time: 1.907	 Loss: 0.01907315
INFO:root:  Epoch 80/150	 Time: 1.922	 Loss: 0.01853061
INFO:root:  Epoch 81/150	 Time: 1.848	 Loss: 0.01936279
INFO:root:  Epoch 82/150	 Time: 1.771	 Loss: 0.01937976
INFO:root:  Epoch 83/150	 Time: 1.718	 Loss: 0.01927372
INFO:root:  Epoch 84/150	 Time: 4.439	 Loss: 0.01931657
INFO:root:  Epoch 85/150	 Time: 1.830	 Loss: 0.01956394
INFO:root:  Epoch 86/150	 Time: 1.724	 Loss: 0.01854451
INFO:root:  Epoch 87/150	 Time: 1.705	 Loss: 0.01976021
INFO:root:  Epoch 88/150	 Time: 1.812	 Loss: 0.01918460
INFO:root:  Epoch 89/150	 Time: 1.896	 Loss: 0.01855083
INFO:root:  Epoch 90/150	 Time: 1.795	 Loss: 0.01942015
INFO:root:  Epoch 91/150	 Time: 3.861	 Loss: 0.01873173
INFO:root:  Epoch 92/150	 Time: 2.456	 Loss: 0.01842132
INFO:root:  Epoch 93/150	 Time: 2.065	 Loss: 0.01833896
INFO:root:  Epoch 94/150	 Time: 1.999	 Loss: 0.01986105
INFO:root:  Epoch 95/150	 Time: 2.057	 Loss: 0.01867485
INFO:root:  Epoch 96/150	 Time: 1.745	 Loss: 0.01802288
INFO:root:  Epoch 97/150	 Time: 1.784	 Loss: 0.01924264
INFO:root:  Epoch 98/150	 Time: 2.806	 Loss: 0.01839287
INFO:root:  Epoch 99/150	 Time: 3.237	 Loss: 0.01920389
INFO:root:  Epoch 100/150	 Time: 1.891	 Loss: 0.01840132
INFO:root:  Epoch 101/150	 Time: 1.945	 Loss: 0.01861382
INFO:root:  Epoch 102/150	 Time: 1.808	 Loss: 0.01871390
INFO:root:  Epoch 103/150	 Time: 1.899	 Loss: 0.01744649
INFO:root:  Epoch 104/150	 Time: 1.688	 Loss: 0.01747273
INFO:root:  Epoch 105/150	 Time: 1.863	 Loss: 0.01758933
INFO:root:  Epoch 106/150	 Time: 4.684	 Loss: 0.01834183
INFO:root:  Epoch 107/150	 Time: 1.902	 Loss: 0.01759182
INFO:root:  Epoch 108/150	 Time: 1.852	 Loss: 0.01667598
INFO:root:  Epoch 109/150	 Time: 1.859	 Loss: 0.01746404
INFO:root:  Epoch 110/150	 Time: 1.730	 Loss: 0.01828573
INFO:root:  Epoch 111/150	 Time: 1.705	 Loss: 0.01787058
INFO:root:  Epoch 112/150	 Time: 1.744	 Loss: 0.01740288
INFO:root:  Epoch 113/150	 Time: 4.111	 Loss: 0.01792400
INFO:root:  Epoch 114/150	 Time: 2.119	 Loss: 0.01785162
INFO:root:  Epoch 115/150	 Time: 1.825	 Loss: 0.01704043
INFO:root:  Epoch 116/150	 Time: 1.827	 Loss: 0.01773820
INFO:root:  Epoch 117/150	 Time: 1.723	 Loss: 0.01708986
INFO:root:  Epoch 118/150	 Time: 1.851	 Loss: 0.01828182
INFO:root:  Epoch 119/150	 Time: 1.726	 Loss: 0.01698881
INFO:root:  Epoch 120/150	 Time: 1.816	 Loss: 0.01675564
INFO:root:  Epoch 121/150	 Time: 4.525	 Loss: 0.01723838
INFO:root:  Epoch 122/150	 Time: 1.723	 Loss: 0.01752926
INFO:root:  Epoch 123/150	 Time: 1.719	 Loss: 0.01802959
INFO:root:  Epoch 124/150	 Time: 1.724	 Loss: 0.01646822
INFO:root:  Epoch 125/150	 Time: 1.788	 Loss: 0.01670755
INFO:root:  Epoch 126/150	 Time: 1.875	 Loss: 0.01716602
INFO:root:  Epoch 127/150	 Time: 1.726	 Loss: 0.01707548
INFO:root:  Epoch 128/150	 Time: 4.854	 Loss: 0.01694369
INFO:root:  Epoch 129/150	 Time: 1.794	 Loss: 0.01687141
INFO:root:  Epoch 130/150	 Time: 1.882	 Loss: 0.01685068
INFO:root:  Epoch 131/150	 Time: 1.832	 Loss: 0.01719189
INFO:root:  Epoch 132/150	 Time: 1.974	 Loss: 0.01705812
INFO:root:  Epoch 133/150	 Time: 1.702	 Loss: 0.01642208
INFO:root:  Epoch 134/150	 Time: 1.706	 Loss: 0.01647033
INFO:root:  Epoch 135/150	 Time: 3.558	 Loss: 0.01614707
INFO:root:  Epoch 136/150	 Time: 3.123	 Loss: 0.01542475
INFO:root:  Epoch 137/150	 Time: 1.838	 Loss: 0.01642454
INFO:root:  Epoch 138/150	 Time: 1.924	 Loss: 0.01616416
INFO:root:  Epoch 139/150	 Time: 1.799	 Loss: 0.01626707
INFO:root:  Epoch 140/150	 Time: 1.713	 Loss: 0.01640189
INFO:root:  Epoch 141/150	 Time: 1.718	 Loss: 0.01702445
INFO:root:  Epoch 142/150	 Time: 1.725	 Loss: 0.01627264
INFO:root:  Epoch 143/150	 Time: 4.595	 Loss: 0.01627293
INFO:root:  Epoch 144/150	 Time: 1.824	 Loss: 0.01533396
INFO:root:  Epoch 145/150	 Time: 1.785	 Loss: 0.01565283
INFO:root:  Epoch 146/150	 Time: 1.786	 Loss: 0.01679872
INFO:root:  Epoch 147/150	 Time: 1.732	 Loss: 0.01716072
INFO:root:  Epoch 148/150	 Time: 1.675	 Loss: 0.01644502
INFO:root:  Epoch 149/150	 Time: 1.722	 Loss: 0.01584953
INFO:root:  Epoch 150/150	 Time: 4.297	 Loss: 0.01693692
INFO:root:Training time: 329.532
INFO:root:Finished training.
INFO:root:Starting testing...
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data
  warnings.warn("test_data has been renamed data")
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:48: UserWarning: test_labels has been renamed targets
  warnings.warn("test_labels has been renamed targets")
INFO:root:Testing time: 2.788
INFO:root:Test set AUC: 87.39%
INFO:root:Finished testing.
/home/liujh/.local/lib/python3.7/site-packages/torchvision/datasets/mnist.py:58: UserWarning: test_data has been renamed data
  warnings.warn("test_data has been renamed data")
